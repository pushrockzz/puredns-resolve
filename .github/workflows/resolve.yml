name: PureDNS Secondary Worker

on:
  workflow_dispatch:
    inputs:
      primary_github_server_url:
        description: 'The server URL of the primary GitHub instance (e.g., https://github.com)'
        required: true
        type: string
      primary_repo_owner:
        description: 'The owner of the primary repository that triggered this workflow.'
        required: true
        type: string
      primary_repo_name:
        description: 'The name of the primary repository that triggered this workflow.'
        required: true
        type: string
      primary_run_id:
        description: 'The run ID of the workflow in the primary repository.'
        required: true
        type: string
      chunk_package_artifact_name:
        description: 'The name of the artifact package containing all chunks and resolvers.'
        required: true
        type: string
      secondary_matrix_json:
        description: 'The JSON string representing the matrix of chunks assigned to this secondary worker.'
        required: true
        type: string

permissions:
  contents: write
  actions: read

env:
  SECONDARY_ACCOUNT_MAX_PARALLEL: 20

jobs:
  process_assigned_chunks_secondary:
    name: Process Assigned Chunks (Secondary Account)
    #if: github.event.inputs.secondary_matrix_json != '' && github.event.inputs.secondary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    strategy:
      fail-fast: false
      max-parallel: 20
      matrix:
        pair: ${{ fromJson(github.event.inputs.secondary_matrix_json && github.event.inputs.secondary_matrix_json || '[]') }}
    steps:

      - name: Display Trigger Payload (Debug)
        run: |
          echo "SECONDARY WORKER: Received payload:"
          echo "${{ toJson(github.event.inputs) }}"
          echo "---"
          echo "SECONDARY WORKER: My assigned matrix for this job instance:"
          echo "${{ toJson(matrix.pair) }}"

      - name: Checkout repository (Account 2's repo)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package from Primary Account
        env:
          GH_TOKEN_PRIMARY_ACCOUNT_READ: ${{ secrets.PAT_FOR_PRIMARY_ACCOUNT_ARTIFACTS_READ }}
          PRIMARY_REPO_OWNER: ${{ github.event.inputs.primary_repo_owner }}
          PRIMARY_REPO_NAME: ${{ github.event.inputs.primary_repo_name }}
          PRIMARY_RUN_ID: ${{ github.event.inputs.primary_run_id }}
          ARTIFACT_NAME_FROM_PRIMARY: ${{ github.event.inputs.chunk_package_artifact_name }}
        shell: bash
        run: |
          echo "SECONDARY WORKER: Downloading artifact '$ARTIFACT_NAME_FROM_PRIMARY' from $PRIMARY_REPO_OWNER/$PRIMARY_REPO_NAME, run ID $PRIMARY_RUN_ID"
          if ! command -v gh &> /dev/null; then
            echo "INFO: gh CLI not found. Installing..."
            apt-get update -qy
            apt-get install -qy curl
            curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
            chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null
            apt-get update -qy
            apt-get install -qy gh
            if ! command -v gh &> /dev/null; then
              echo "ERROR: gh CLI installation failed."
              exit 1
            fi
          fi

          echo "$GH_TOKEN_PRIMARY_ACCOUNT_READ" | gh auth login --with-token
          FULL_PRIMARY_REPO="$PRIMARY_REPO_OWNER/$PRIMARY_REPO_NAME"
          gh run download "$PRIMARY_RUN_ID" -R "$FULL_PRIMARY_REPO" -n "$ARTIFACT_NAME_FROM_PRIMARY" --dir .

          PACKAGE_FILENAME="$ARTIFACT_NAME_FROM_PRIMARY.tar.gz"
          if [ ! -f "$PACKAGE_FILENAME" ]; then
            echo "ERROR: Failed to download '$PACKAGE_FILENAME'."
            exit 1
          fi
          echo "Downloaded '$PACKAGE_FILENAME'."

      - name: Extract Chunks and Resolvers for Secondary
        shell: bash
        run: |
          PACKAGE_FILENAME="${{ github.event.inputs.chunk_package_artifact_name }}.tar.gz"
          echo "SECONDARY WORKER: Extracting $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ] || [ ! -f "resolvers.txt" ]; then
            echo "ERROR: 'chunks/' or 'resolvers.txt' missing after extraction."
            exit 1
          fi
          echo "Extraction complete."
          ls -R chunks/
          ls resolvers.txt resolvers-trusted.txt

      - name: Install dsieve
        run: |
          if command -v dsieve &> /dev/null; then
            echo "dsieve is already installed"
          else
            echo "Installing dsieve..."
            go install github.com/trickest/dsieve@latest
          fi

      - name: Install Tools
        run: |
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smap…"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already in cache"
          fi
          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi

          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi

          if ! command -v cut-cdn >/dev/null; then
            echo "Installing cut-cdn…"
            go install github.com/ImAyrix/cut-cdn@latest
          else
            echo "cut-cdn already in cache"
          fi

          if ! command -v naabu >/dev/null; then
            echo "Installing naabu…"
            go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest
          else
            echo "naabu already in cache"
          fi

          pip3 install --no-cache-dir ipaddress

          echo "$HOME/go/bin" >> $GITHUB_PATH

      - name: Fetch wordlists
        shell: bash
        run: |

          if [ ! -f resolvers.txt ]; then
              wget -qO resolvers.txt \
              https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
              echo "resolvers.txt is downloaded"
          fi
          if [ ! -f resolvers-trusted.txt ]; then
              wget -qO resolvers-trusted.txt \
              https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
              echo "resolvers-trusted.txt is downloaded"
          fi

      - name: Run puredns + pre-dsieve on subdomains + final filtering
        shell: bash
        run: |
          set -euo pipefail
          trap '' SIGPIPE

          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}
          echo "Processing chunk '$CHUNK_FILE_PATH'..."
          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            exit 1
          fi

          # ====================================================================
          # STAGE 1: PUREDNS RESOLUTION (Naming now matches reference)
          # ====================================================================
          # Define filenames to match the reference workflow
          PUREDNS_FILE="puredns_file.txt"
          MASSDNS="massdns.txt"
          MASSDNS_FILE="massdns_file.txt"
          PARENT_DOMAINS_SCOPE="parent_domains.scope" # This is the one necessary new variable

          echo "-> Generating a dynamic scope file from the raw chunk..."
          dsieve -if "$CHUNK_FILE_PATH" -f 2 | sort -u > "$PARENT_DOMAINS_SCOPE"
          echo "  -> Scope for this chunk contains $(wc -l < "$PARENT_DOMAINS_SCOPE") parent domains."

          echo "-> Running puredns..."
          puredns resolve "$CHUNK_FILE_PATH" \
            -r resolvers.txt \
            --rate-limit 3000 \
            --skip-validation \
            --skip-wildcard-filter \
            --write "$PUREDNS_FILE" \
            --write-massdns "$MASSDNS" \
            --quiet >/dev/null 2>&1

          if [ ! -s "$MASSDNS" ]; then
              echo "INFO: Puredns found no resolvable hosts in this chunk. Exiting."
              mkdir -p results
              exit 0
          fi

          # ====================================================================
          # STAGE 2: CLEAN AND FILTER MASS_DNS DATA (Logic and Naming now match reference)
          # ====================================================================
          TMP_CLEANMASSDNS=$(mktemp)

          echo "-> Cleaning raw massdns output ('$MASSDNS')..."
          awk 'NF { sub(/\.$/,"",$1); print }' "$MASSDNS" > "$TMP_CLEANMASSDNS"

          echo "-> Filtering against the dynamic scope to create '$MASSDNS_FILE'..."
          awk '
          {gsub(/\r$/,"");sub(/^[ \t]+/,"");sub(/[ \t]+$/,"")}
          FNR==NR{if($0)patterns[++c]=$0;next}
          !setup{regex="";for(i=1;i<=c;i++){regex=regex (i>1?"|":"") "("patterns[i]")"};if(regex=="")regex="^\b$";setup=1}
          $2=="A" && $1~regex
          ' "$PARENT_DOMAINS_SCOPE" "$TMP_CLEANMASSDNS" | anew -q "$MASSDNS_FILE"
          echo "  -> Found $(wc -l < "$MASSDNS_FILE") in-scope A records."

          # ====================================================================
          # STAGE 3: MAP SUBDOMAINS TO PORTS (Logic and Naming now match reference)
          # ====================================================================
          OUTPUT="subdomain_ports.txt"
          SMAP_FILE="smap.txt"
          TMP_IP2SUB=$(mktemp)
          TMP_IP_ONLY=$(mktemp)
          TMP_NONCDN=$(mktemp)
          TMP_CDN=$(mktemp)
          TMP_SMAP_NONCDN=$(mktemp)
          TMP_RUSTSCAN=$(mktemp)

          echo "-> Extracting IPs from '$MASSDNS_FILE'..."
          awk '{ print $3, $1 }' "$MASSDNS_FILE" | sort -k1,1 > "$TMP_IP2SUB"
          cut -d' ' -f1 "$TMP_IP2SUB" | sort -u > "$TMP_IP_ONLY"

          echo "-> Filtering out CDN IPs..."
          cat "$TMP_IP_ONLY" | cut-cdn -ua -t 50 -silent -o "$TMP_NONCDN"
          cat "$TMP_IP_ONLY" | anew -d "$TMP_NONCDN" > "$TMP_CDN"

          echo "-> Scanning non-CDN IPs for ports 80, 443..."
          smap -iL "$TMP_NONCDN" -oP "$TMP_SMAP_NONCDN" || true
          rustscan -a "$TMP_NONCDN" -p 70,80,81,82,83,84,85,88,443,888,1234,1311,2002,3000,3128,4443,5000,5222,5269,5800,7070,8000,8001,8002,8003,8004,8006,8008,8009,8010,8080,8081,8082,8083,8084,8085,8088,8181,8443,8880,8887,8888,9000,9001,9080,9090,9999,10000,10001,50000 --no-banner -t 1000 --tries 1 -u 20000 --scan-order "Random" -b 100 --greppable --accessible > "$TMP_RUSTSCAN" || true
          cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }' | anew -q "$TMP_SMAP_NONCDN"
          cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }'

          cat "$TMP_SMAP_NONCDN" "$TMP_CDN" | sort -u > "$SMAP_FILE"

          echo "-> Showing you SMAP_FILE---------"
          cat "$SMAP_FILE"

          echo "-> Joining IPs/Ports to create '$OUTPUT'..."
          awk -F: '
            NF==2 { print $1, $2 }
            NF==1 { print $1, ""  }
          ' "$SMAP_FILE" \
            | sort -k1,1 \
            | join - "$TMP_IP2SUB" \
            | {
              awk '
                NF >= 2 {
                  if (NF == 3 && $2 ~ /^[0-9]+$/) {
                    print $3 ":" $2
                  } else {
                    print $NF
                  }
                }
              '
            } \
            > "$OUTPUT"
          echo "  -> Generated enriched list with $(wc -l < "$OUTPUT") entries."

          cat "$OUTPUT"

          # ====================================================================
          # STAGE 4: SORT FINAL RESULTS INTO ROOT DOMAIN FOLDERS
          # ====================================================================
          OUTPUT_ROOT="results"
          mkdir -p "$OUTPUT_ROOT"
          echo "-> Sorting all results into per-domain folders..."
          while read -r parent; do
            clean_parent=$(printf '%s' "$parent" | tr -d '\r' | xargs)
            if [ -z "$clean_parent" ]; then continue; fi
            mkdir -p "$OUTPUT_ROOT/$clean_parent"

            # Sort the simple puredns results (This file doesn't exist in the reference, but we keep it for extra data)
            simple_outfile="$OUTPUT_ROOT/$clean_parent/puredns_results.txt"
            grep -E "(^|\\.)${clean_parent//./\\.}(\$)" "$PUREDNS_FILE"  | anew -q "$simple_outfile" || true

            # Sort the enriched subdomain:port results
            ports_outfile="$OUTPUT_ROOT/$clean_parent/subdomain_ports.txt"
            grep -E "(^|\\.)${clean_parent//./\\.}(\:|\$)" "$OUTPUT" --color=never | anew -q "$ports_outfile" || true
          done < "$PARENT_DOMAINS_SCOPE"

          # ====================================================================
          # STAGE 5: CLEANUP (Matches reference)
          # ====================================================================
          rm -f "$TMP_CLEANMASSDNS" "$MASSDNS" "$PUREDNS_FILE"
          rm -f "$TMP_IP2SUB" "$TMP_IP_ONLY" "$TMP_NONCDN" "$TMP_CDN" "$TMP_SMAP_NONCDN" "$TMP_RUSTSCAN"

      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV

      - name: Upload Secondary Account Results
        uses: actions/upload-artifact@v4
        with:
          name: recon-results-secondary-${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1

  # ====================================================================
  # JOB 4: Merge All Results (Identical to reference workflow)
  # ====================================================================
  merge_results:
    name: Merge All Distributed Results
    runs-on: ubuntu-latest
    needs: process_assigned_chunks_secondary    
    if: always()
    outputs:
      has_results: ${{ steps.consolidate.outputs.has_results }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Download all result artifacts from all accounts
        uses: actions/download-artifact@v4
        with:
          pattern: 'recon-results-*'
          path: temp-aggregated-results
          merge-multiple: true
      - name: Consolidate all results into root domain folders
        id: consolidate
        shell: bash
        run: |
          set -e
          mkdir -p final_results
          if [ ! -d "temp-aggregated-results" ] || [ -z "$(ls -A temp-aggregated-results)" ]; then
            echo "::warning:: No result artifacts were found. Nothing to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "INFO: Aggregating all downloaded results..."
          for filepath in $(find temp-aggregated-results -type f -name "puredns_results.txt"); do
            parent_domain=$(basename "$(dirname "$filepath")")
            dest_file="final_results/$parent_domain/puredns_results.txt"
            mkdir -p "final_results/$parent_domain"
            cat "$filepath" >> "$dest_file"
          done
          echo "INFO: Aggregating subdomain port data..."
          for filepath in $(find temp-aggregated-results -type f -name "subdomain_ports.txt"); do
            parent_domain=$(basename "$(dirname "$filepath")")
            dest_file="final_results/$parent_domain/subdomain_ports.txt"
            mkdir -p "final_results/$parent_domain"
            cat "$filepath" >> "$dest_file"
          done
          echo "INFO: De-duplicating all aggregated files..."
          for final_file in $(find final_results -type f -name "*.txt"); do
              sort -u -o "$final_file" "$final_file"
          done
          if [ -z "$(ls -A final_results)" ]; then
            echo "::warning:: Result artifacts contained no valid data to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "has_results=true" >> $GITHUB_OUTPUT
          echo "✅ Successfully consolidated results from all accounts."
          ls -R final_results
      - name: Upload Final Consolidated Artifact
        if: steps.consolidate.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results/
          retention-days: 1

  commit_all_secondary_results:
    name: Commit All Results
    needs: merge_results
    # This job only runs if the merge_results job actually produced data.
    if: needs.merge_results.outputs.has_results == 'true'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    steps:
      - name: Download the single consolidated results artifact
        uses: actions/download-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results

      - name: Organize and Push to store-recon
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDARY }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{secrets.STORE}}
          CORRELATION_ID: ${{ github.event.inputs.primary_run_id }}
        run: |
          set -e
          RESULTS_DIR="${GITHUB_WORKSPACE}/final_results"
          if [ ! -d "$RESULTS_DIR" ] || [ -z "$(ls -A "$RESULTS_DIR")" ]; then
            echo "::warning:: Results directory is empty or does not exist. Nothing to commit."
            exit 0
          fi
          echo "Cloning ${{ env.STORE }} to commit results..."
          git config --global user.name "Puredns-Permut Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          TMP_DIR="$(mktemp -d)"
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"
          cd "$TMP_DIR"
          echo "Merging new consolidated results into the repository..."
          for domain_dir in "${RESULTS_DIR}"/*; do
            if [ ! -d "$domain_dir" ]; then continue; fi
            domain_name=$(basename "$domain_dir")
            dest_repo_dir="results/$domain_name"
            mkdir -p "$dest_repo_dir"

            # --- MERGE BLOCK 1 (Identical to reference): puredns_results.txt -> all_subdomains.txt ---
            source_puredns_file="$domain_dir/puredns_results.txt"
            dest_all_subs_file="$dest_repo_dir/all_subdomains.txt"
            if [ -s "$source_puredns_file" ]; then
              echo "  -> Merging simple results into '$dest_all_subs_file'"
              temp_merged_file_1=$(mktemp)
              if [ -f "$dest_all_subs_file" ]; then
                cat "$source_puredns_file" "$dest_all_subs_file" | sort -u > "$temp_merged_file_1"
              else
                sort -u "$source_puredns_file" > "$temp_merged_file_1"
              fi
              mv "$temp_merged_file_1" "$dest_all_subs_file"
            fi

            # --- MERGE BLOCK 2 (Identical to reference): subdomain_ports.txt -> puredns_result.txt ---
            source_ports_file="$domain_dir/subdomain_ports.txt"
            dest_puredns_file="$dest_repo_dir/puredns_result.txt"
            if [ -s "$source_ports_file" ]; then
              echo "  -> Merging enriched port data into '$dest_puredns_file'"
              temp_merged_file_2=$(mktemp)
              if [ -f "$dest_puredns_file" ]; then
                cat "$source_ports_file" "$dest_puredns_file" | sort -u > "$temp_merged_file_2"
              else
                sort -u "$source_ports_file" > "$temp_merged_file_2"
              fi
              mv "$temp_merged_file_2" "$dest_puredns_file"
            fi
          done

          if git diff --cached --quiet && git diff --quiet; then
            echo "No new unique data to commit in ${{ env.STORE }}."
            exit 0
          fi
          echo "Committing and pushing changes to ${{ env.STORE }}..."
          git add results/
          git commit -m "feat: Add new assets from distributed permutation scan with Correlation ID: ${CORRELATION_ID}"
          git push -v origin main
          echo "✅ Successfully pushed new consolidated results to ${{ env.STORE }}."
